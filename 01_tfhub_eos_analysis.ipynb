{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca49962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_tfhub_eos_analysis.ipynb\n",
    "\n",
    "# Goal:\n",
    "# Analyze which tokens from the TFHub Sentence-T5 embedding space are closest to the `</s>` token.\n",
    "# Discover soft-`</s>` tokens (e.g., lucrarea) and explain their effect on SCS scoring.\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Needed to register SentencePiece ops\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence-T5 encoder from TFHub\n",
    "model_url = \"t5\"\n",
    "encoder = hub.KerasLayer(model_url)\n",
    "\n",
    "# Load SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"t5/spiece.model\")  # Adjust path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get embedding for a text input\n",
    "def get_embed(text):\n",
    "    output = encoder(tf.constant([text]))\n",
    "    if isinstance(output, list):\n",
    "        output = output[0]\n",
    "    elif isinstance(output, dict):\n",
    "        output = list(output.values())[0]\n",
    "    return output.numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all token embeddings in batches\n",
    "def get_all_token_embeddings(encoder, token_texts, batch_size=512):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(token_texts), batch_size)):\n",
    "        batch = tf.constant(token_texts[i:i+batch_size])\n",
    "        output = encoder(batch)\n",
    "        if isinstance(output, list):\n",
    "            output = output[0]\n",
    "        elif isinstance(output, dict):\n",
    "            output = list(output.values())[0]\n",
    "        all_embeddings.append(output.numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Sharpened cosine and raw dot product similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c414275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(batch_embeds, target_embed, p=3):\n",
    "    # Cosine similarity\n",
    "    cos_sim = cosine_similarity(batch_embeds, target_embed.reshape(1, -1)).flatten()\n",
    "    sharp_cos = cos_sim ** p\n",
    "\n",
    "    # Raw dot product\n",
    "    raw_dot = np.dot(batch_embeds, target_embed)\n",
    "    return sharp_cos, raw_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens\n",
    "token_texts = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "all_token_embeddings = get_all_token_embeddings(encoder, token_texts, batch_size=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86300de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fake eos from parts\n",
    "parts = [\"<\", \"/\", \"s\", \">\"]\n",
    "eos_fake = np.mean([get_embed(p) for p in parts], axis=0)\n",
    "eos_fake /= np.linalg.norm(eos_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarities\n",
    "cosine_sim, dot_sim = compute_similarities(all_token_embeddings, eos_fake, p=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and show\n",
    "top_k = 100\n",
    "top_indices = np.argsort(cosine_sim)[::-1][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aaa846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top tokens closest to </s> (cosine & dot product):\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{token_texts[idx]:<15} | cosine: {cosine_sim[idx]:.4f} | dot: {dot_sim[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe463208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4e3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st5env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
